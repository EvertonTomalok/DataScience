{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cbow:\n",
    "    def __init__(self, text: str, context_window: int=1):\n",
    "        self._text = text\n",
    "        self._tokens = word_tokenize(text)\n",
    "        self._len_tokens = len(self._tokens)\n",
    "        self._context_window = context_window\n",
    "        self.id2word = {\n",
    "            _id+1: text \n",
    "            for _id, text in enumerate(set(self.tokens))\n",
    "        }\n",
    "        self.id2word[0] = \"PAD\"\n",
    "        self.vocab_size = len(self.id2word)\n",
    "        self.word2id = {v: k for k, v in self.id2word.items()}\n",
    "        \n",
    "    @property\n",
    "    def text(self):\n",
    "        return self._text\n",
    "    \n",
    "    @property\n",
    "    def len_tokens(self):\n",
    "        return self._len_tokens\n",
    "    \n",
    "    @property\n",
    "    def tokens(self):\n",
    "        return self._tokens\n",
    "    \n",
    "    @property\n",
    "    def context_window(self):\n",
    "        return self._context_window\n",
    "    \n",
    "    def get_context_words(self):\n",
    "        for index, word in enumerate(self.tokens):\n",
    "                        \n",
    "            start = index - self.context_window\n",
    "            end = index + self.context_window + 1\n",
    "            \n",
    "            if index - self.context_window < 0:\n",
    "                continue\n",
    "            elif index + self.context_window == self.len_tokens:\n",
    "                break\n",
    "                \n",
    "            word_list = [\n",
    "                self.tokens[i] for i in range(start, end)\n",
    "                if i != index\n",
    "            ]\n",
    "            \n",
    "            yield (\n",
    "                np.array([1 if index == i else 0 for i in range(self.vocab_size)]).reshape(1, self.vocab_size), \n",
    "                np.array([self.word2id[word] for word in word_list]).reshape(1,2)\n",
    "            )\n",
    "\n",
    "    def train(self, epochs=10):\n",
    "        vocab_size = len(self.id2word)\n",
    "        embed_size = 100\n",
    "        cbow = Sequential()\n",
    "        cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=self._context_window*2))\n",
    "        cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "        cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "        cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            loss = 0.\n",
    "            i = 0\n",
    "            for y, x in self.get_context_words():\n",
    "                i += 1\n",
    "                loss += cbow.train_on_batch(x, y)\n",
    "                if i % 100000 == 0:\n",
    "                    print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "            print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "            print()\n",
    "        \n",
    "        weights = cbow.get_weights()[0]\n",
    "        weights = weights[1:]\n",
    "        \n",
    "        return weights\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texto_dom_casmurro = \"\"\"\n",
    "Publicado pela primeira vez em 1899, “Dom Casmurro” é uma das grandes obras de Machado de Assis e confirma o olhar certeiro e crítico que o autor estendia sobre toda a sociedade brasileira. Também a temática do ciúme, abordada com brilhantismo nesse livro, provoca polêmicas em torno do caráter de uma das principais personagens femininas da literatura brasileira: Capitu.\n",
    "O romance inicia-se numa situação posterior a todos os seus acontecimentos. Bento Santiago, já um homem de idade, conta ao leitor como recebeu a alcunha de Dom Casmurro. A expressão fora inventada por um jovem poeta, que tentara ler para ele no trem alguns de seus versos. Como Bento cochilara durante a leitura, o rapaz ficou chateado e começou a chamá-lo daquela forma.\n",
    "Bentinho vai estudar direito no Largo de São Francisco, em São Paulo. Quando conclui os estudos, torna-se o doutor Bento de Albuquerque Santiago. Ocorre então o casamento tão esperado entre Bento e Capitu. Escobar, por seu lado, casara-se com Sancha, uma antiga amiga de colégio de Capitu. Capitu e Bentinho formam um “duo afinadíssimo”.\n",
    "\n",
    "Essa felicidade, entretanto, começa a ser ameaçada com a demora do casal em ter um filho. Escobar e Sancha não encontram a mesma dificuldade: têm uma bela menina, a quem colocam o nome de Capitolina.\n",
    "\n",
    "Depois de alguns anos, Capitu finalmente tem um filho, e o casal pode retribuir a homenagem que Escobar e Sancha lhe haviam prestado: o filho é batizado com o nome de Ezequiel.\n",
    "\n",
    "Os casais passam a conviver intensamente. Bento vê uma semelhança terrível entre o pequeno Ezequiel e seu amigo Escobar, que, numa de suas aventuras na praia – o personagem era excelente nadador -, morre afogado.\n",
    "\n",
    "Bento enxerga no filho a figura do amigo falecido e fica convencido de que fora traído pela mulher. Resolve suicidar-se bebendo uma xícara de café envenenado. Quando Ezequiel entra em seu escritório, decide matar a criança, mas desiste no último momento. Diz ao garoto, então, que não é seu pai. Capitu escuta tudo e lamenta-se pelo ciúme de Bentinho, que, segundo ela, fora despertado pela casualidade da semelhança.\n",
    "\n",
    "Após inúmeras discussões, o casal decide separar-se. Arruma-se uma viagem para a Europa com o intuito de encobrir a nova situação, que levantaria muita polêmica. O protagonista retorna sozinho ao Brasil e se torna, pouco a pouco, o amargo Dom Casmurro. Capitu morre no exterior e Ezequiel tenta reatar relações com ele, mas a semelhança extrema com Escobar faz com que Bento Santiago o rejeite novamente. O destino de Ezequiel é infeliz: ele morre de febre tifóide durante uma pesquisa arqueológica em Jerusalém.\n",
    "\n",
    "Triste e nostálgico, o narrador constrói uma casa que imita sua casa de infância, na rua de Matacavalos. O próprio livro é também uma tentativa de recuperar o sentido de sua vida. No fim, o narrador parece menosprezar um pouco a própria criação. Convence-se de que o melhor a fazer agora é escrever outra obra sobre “a história dos subúrbios”.\n",
    "\n",
    "Lista de personagens\n",
    "Bentinho (Bento Santiago): o narrador-personagem que conta suas memórias, membro da elite carioca do século XIX.\n",
    "\n",
    "Capitu (Capitolina): grande amor de Bentinho, personagem de origem pobre, mas independente e avançada.\n",
    "\n",
    "Escobar: o melhor amigo de Bentinho, a quem conheceu quando estudaram juntos no seminário.\n",
    "\n",
    "Dona Sancha: mulher de Escobar, ex-colega de colégio de Capitu.\n",
    "\n",
    "Dona Glória: mãe de Bentinho, adora o filho e é também muito religiosa. Quer que o garoto se ordene padre como cumprimento de uma promessa que fez.\n",
    "\n",
    "José Dias: agregado que vive de favores na casa de dona Glória. Suposto médico, tem o hábito de agradar aos proprietários da casa com o uso de superlativos.\n",
    "\n",
    "Tio Cosme: irmão de dona Glória, viúvo e advogado.\n",
    "\n",
    "Prima Justina: prima de dona Glória, que, segundo o narrador, não tinha papas na língua.\n",
    "\n",
    "Pedro de Albuquerque Santiago: pai de Bentinho, faleceu quando o filho ainda era muito pequeno.\n",
    "\n",
    "Senhor Pádua e Dona Fortunata: pais de Capitu, que viam no possível casamento da filha com Bentinho uma possibilidade de ascensão social.\n",
    "\n",
    "Ezequiel: filho de Capitu, sobre o qual o narrador sustenta forte dúvida quanto à paternidade, pois o garoto tinha grande semelhança física com Escobar.\n",
    "\n",
    "Sobre Machado de Assis\n",
    "Joaquim Maria Machado de Assis nasceu em 21 de junho de 1839 na cidade do Rio de Janeiro. Neto de escravos alforriados, foi criado em uma família pobre e não pode frequentar regularmente a escola. Porém, devido a seu enorme interesse por literatura, conseguiu se instruir por conta própria. Entre os seis e os quatorze anos, Machado de Assis perdeu sua irmã, a mãe e o pai.\n",
    "\n",
    "Aos 16 anos, Machado conseguiu um emprego como aprendiz em uma tipografia, vindo a publicar seus primeiros versos no jornal “A Marmota”. Em 1860 passou a colaborar para o “Diário do Rio de Janeiro” e é dessa década que datam quase todas suas comédias teatrais e “Crisálidas”, um livro de poemas.\n",
    "\n",
    "Em 1869 Machado de Assis casou-se com Carolina Augusta Xavier de Novais sem o consentimento da família da moça, devido à má fama que Machado carregava. Porém, este casamento mudou sua vida, uma vez que Carolina lhe apresentou à literatura portuguesa e inglesa. Mais amadurecido literariamente, Machado publica na década de 1870 uma série de romances, tais como “A mão e a luva” (1874) e “Helena” (1876), vindo a obter reconhecimento do público e da crítica. Ainda na década de 1870, Machado iniciou sua carreira burocrática e em 1892 já ocupava o cargo de diretor geral do Ministério da Aviação. Através de sua carreira no serviço público, Machado de Assis conseguiu sua estabilidade financeira.\n",
    "\n",
    "A obra literária de Machado era marcadamente romântica, mas na década de 1880 ela sofre uma grande mudança estilística e temática, vindo a inaugurar o Realismo no Brasil com a publicação de “Memórias Póstumas de Brás Cubas” (1881). A partir de então a ironia, o pessimismo, o espírito crítico e uma profunda reflexão sobre a sociedade brasileira se tornarão as principais características de suas obras. Em 1897, Machado funda a Academia Brasileira de Letras, sendo seu primeiro presidente e ocupando a Cadeira Nº 23.\n",
    "\n",
    "Em 1904, Machado perde a esposa após um casamento de 35 anos. A morte de Carolina abalou profundamente o escritor, que passou a ficar isolado em casa e sua saúde foi piorando. Dessa época datam seus últimos romances: “Esaú e Jacó” (1904) e “Memorial de Aires” (1908). Machado morreu em sua casa no Rio de Janeiro no dia 29 de setembro de 1908. Seu enterro foi acompanhado por uma multidão e foi decretado luto oficial no Rio de Janeiro.\n",
    "\n",
    "Seus principais romances são: “Ressurreição” (1872), “A mão e a luva” (1874), “Helena” (1876), “Iaiá Garcia” (1878), “Memórias Póstumas de Brás Cubas” (1881), “Quincas Borba” (1891), “Dom Casmurro” (1899), “Esaú e Jacó” (1904) e “Memorial de Aires” (1908). Além dessas obras, Machado de Assis possui uma extensa bibliografia que abrange poemas, contos e peças teatrais.\n",
    "\"\"\"\n",
    "\n",
    "cbow = Cbow(texto_dom_casmurro, context_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 3559.0491003990173\n",
      "\n",
      "Epoch: 1 \tLoss: 3546.9125423431396\n",
      "\n",
      "Epoch: 2 \tLoss: 3533.994533061981\n",
      "\n",
      "Epoch: 3 \tLoss: 3518.9297347068787\n",
      "\n",
      "Epoch: 4 \tLoss: 3501.50573015213\n",
      "\n",
      "Epoch: 5 \tLoss: 3481.918444633484\n",
      "\n",
      "Epoch: 6 \tLoss: 3460.4874210357666\n",
      "\n",
      "Epoch: 7 \tLoss: 3437.309811115265\n",
      "\n",
      "Epoch: 8 \tLoss: 3412.444531917572\n",
      "\n",
      "Epoch: 9 \tLoss: 3385.848226070404\n",
      "\n",
      "Epoch: 10 \tLoss: 3357.6673288345337\n",
      "\n",
      "Epoch: 11 \tLoss: 3328.022168159485\n",
      "\n",
      "Epoch: 12 \tLoss: 3297.124123573303\n",
      "\n",
      "Epoch: 13 \tLoss: 3265.2039289474487\n",
      "\n",
      "Epoch: 14 \tLoss: 3232.4600682258606\n",
      "\n",
      "Epoch: 15 \tLoss: 3199.0436730384827\n",
      "\n",
      "Epoch: 16 \tLoss: 3165.063704967499\n",
      "\n",
      "Epoch: 17 \tLoss: 3130.752815246582\n",
      "\n",
      "Epoch: 18 \tLoss: 3096.2177872657776\n",
      "\n",
      "Epoch: 19 \tLoss: 3061.5682356357574\n",
      "\n",
      "Epoch: 20 \tLoss: 3026.8437275886536\n",
      "\n",
      "Epoch: 21 \tLoss: 2992.1865453720093\n",
      "\n",
      "Epoch: 22 \tLoss: 2957.947756767273\n",
      "\n",
      "Epoch: 23 \tLoss: 2924.3002395629883\n",
      "\n",
      "Epoch: 24 \tLoss: 2891.431887626648\n",
      "\n",
      "Epoch: 25 \tLoss: 2859.6070630550385\n",
      "\n",
      "Epoch: 26 \tLoss: 2828.7914848327637\n",
      "\n",
      "Epoch: 27 \tLoss: 2799.1851000785828\n",
      "\n",
      "Epoch: 28 \tLoss: 2770.8647191524506\n",
      "\n",
      "Epoch: 29 \tLoss: 2743.725575685501\n",
      "\n",
      "Epoch: 30 \tLoss: 2717.7544453144073\n",
      "\n",
      "Epoch: 31 \tLoss: 2692.8108234405518\n",
      "\n",
      "Epoch: 32 \tLoss: 2668.8291602134705\n",
      "\n",
      "Epoch: 33 \tLoss: 2645.791480779648\n",
      "\n",
      "Epoch: 34 \tLoss: 2623.586161375046\n",
      "\n",
      "Epoch: 35 \tLoss: 2602.0992562770844\n",
      "\n",
      "Epoch: 36 \tLoss: 2581.237686276436\n",
      "\n",
      "Epoch: 37 \tLoss: 2560.899335026741\n",
      "\n",
      "Epoch: 38 \tLoss: 2540.9636124372482\n",
      "\n",
      "Epoch: 39 \tLoss: 2521.4548820257187\n",
      "\n",
      "Epoch: 40 \tLoss: 2502.396493434906\n",
      "\n",
      "Epoch: 41 \tLoss: 2483.8233567476273\n",
      "\n",
      "Epoch: 42 \tLoss: 2465.723869919777\n",
      "\n",
      "Epoch: 43 \tLoss: 2448.110762834549\n",
      "\n",
      "Epoch: 44 \tLoss: 2431.053205370903\n",
      "\n",
      "Epoch: 45 \tLoss: 2414.5592510700226\n",
      "\n",
      "Epoch: 46 \tLoss: 2398.6439864635468\n",
      "\n",
      "Epoch: 47 \tLoss: 2383.306398868561\n",
      "\n",
      "Epoch: 48 \tLoss: 2368.3754173517227\n",
      "\n",
      "Epoch: 49 \tLoss: 2353.928955078125\n",
      "\n",
      "Epoch: 50 \tLoss: 2339.9109988212585\n",
      "\n",
      "Epoch: 51 \tLoss: 2326.242888212204\n",
      "\n",
      "Epoch: 52 \tLoss: 2312.883910179138\n",
      "\n",
      "Epoch: 53 \tLoss: 2299.7858313322067\n",
      "\n",
      "Epoch: 54 \tLoss: 2287.0187915563583\n",
      "\n",
      "Epoch: 55 \tLoss: 2274.5906463861465\n",
      "\n",
      "Epoch: 56 \tLoss: 2262.447472333908\n",
      "\n",
      "Epoch: 57 \tLoss: 2250.616572380066\n",
      "\n",
      "Epoch: 58 \tLoss: 2239.014919102192\n",
      "\n",
      "Epoch: 59 \tLoss: 2227.647900760174\n",
      "\n",
      "Epoch: 60 \tLoss: 2216.4211698174477\n",
      "\n",
      "Epoch: 61 \tLoss: 2205.372933268547\n",
      "\n",
      "Epoch: 62 \tLoss: 2194.538218796253\n",
      "\n",
      "Epoch: 63 \tLoss: 2183.929819226265\n",
      "\n",
      "Epoch: 64 \tLoss: 2173.541212141514\n",
      "\n",
      "Epoch: 65 \tLoss: 2163.406673669815\n",
      "\n",
      "Epoch: 66 \tLoss: 2153.494989693165\n",
      "\n",
      "Epoch: 67 \tLoss: 2143.7256683707237\n",
      "\n",
      "Epoch: 68 \tLoss: 2133.992642223835\n",
      "\n",
      "Epoch: 69 \tLoss: 2124.2442549467087\n",
      "\n",
      "Epoch: 70 \tLoss: 2114.4636038541794\n",
      "\n",
      "Epoch: 71 \tLoss: 2104.6534852981567\n",
      "\n",
      "Epoch: 72 \tLoss: 2094.9264363646507\n",
      "\n",
      "Epoch: 73 \tLoss: 2085.3282269239426\n",
      "\n",
      "Epoch: 74 \tLoss: 2075.900854408741\n",
      "\n",
      "Epoch: 75 \tLoss: 2066.629478275776\n",
      "\n",
      "Epoch: 76 \tLoss: 2057.485369026661\n",
      "\n",
      "Epoch: 77 \tLoss: 2048.4826323986053\n",
      "\n",
      "Epoch: 78 \tLoss: 2039.6424179077148\n",
      "\n",
      "Epoch: 79 \tLoss: 2031.0685150921345\n",
      "\n",
      "Epoch: 80 \tLoss: 2022.5356055498123\n",
      "\n",
      "Epoch: 81 \tLoss: 2014.1687489748\n",
      "\n",
      "Epoch: 82 \tLoss: 2005.9748557507992\n",
      "\n",
      "Epoch: 83 \tLoss: 1997.938656449318\n",
      "\n",
      "Epoch: 84 \tLoss: 1990.100660175085\n",
      "\n",
      "Epoch: 85 \tLoss: 1982.442384004593\n",
      "\n",
      "Epoch: 86 \tLoss: 1974.9404774308205\n",
      "\n",
      "Epoch: 87 \tLoss: 1967.68757507205\n",
      "\n",
      "Epoch: 88 \tLoss: 1960.700239032507\n",
      "\n",
      "Epoch: 89 \tLoss: 1953.939847111702\n",
      "\n",
      "Epoch: 90 \tLoss: 1947.4681552648544\n",
      "\n",
      "Epoch: 91 \tLoss: 1941.3124252557755\n",
      "\n",
      "Epoch: 92 \tLoss: 1935.3405636399984\n",
      "\n",
      "Epoch: 93 \tLoss: 1929.5435704886913\n",
      "\n",
      "Epoch: 94 \tLoss: 1923.979763403535\n",
      "\n",
      "Epoch: 95 \tLoss: 1918.6739515960217\n",
      "\n",
      "Epoch: 96 \tLoss: 1913.5516857653856\n",
      "\n",
      "Epoch: 97 \tLoss: 1908.5469118654728\n",
      "\n",
      "Epoch: 98 \tLoss: 1903.5801217556\n",
      "\n",
      "Epoch: 99 \tLoss: 1898.5851909220219\n",
      "\n",
      "(559, 100)\n"
     ]
    }
   ],
   "source": [
    "w = cbow.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(559, 559)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'filho': ['tem', 'enxerga', 'figura', 'finalmente', 'terrível'],\n",
       " 'romances': ['grande', 'muito', 'aprendiz', '1876', 'Iaiá'],\n",
       " 'mão': ['membro', 'possibilidade', 'família', 'pessimismo', '29'],\n",
       " 'luva': ['aos', '1891', 'regularmente', 'uso', 'isolado']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute pairwise distance matrix\n",
    "distance_matrix = euclidean_distances(w)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "# view contextually similar words\n",
    "similar_words = {search_term: [cbow.id2word[idx] for idx in distance_matrix[cbow.word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['filho', 'romances', 'mão', 'luva']}\n",
    "\n",
    "similar_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoje\n",
      "é\n",
      "terça\n",
      "-\n",
      "feira\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from string import punctuation\n",
    "from collections import Counter, OrderedDict\n",
    "from spacy.symbols import ORTH, LEMMA, POS, TAG\n",
    "\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "for n in nlp('Hoje é terça-feira'):\n",
    "    print(n.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando casos especiais de tokens\n",
    "\n",
    "#### Com regex pura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hoje',\n",
       " 'é',\n",
       " 'terça-feira',\n",
       " '.',\n",
       " 'Segunda-feira',\n",
       " 'fui',\n",
       " 'ao',\n",
       " 'médico',\n",
       " 'e',\n",
       " 'o',\n",
       " 'Dr',\n",
       " '.',\n",
       " 'Arnaldo',\n",
       " 'em',\n",
       " 'receitou',\n",
       " 'um',\n",
       " 'remédio',\n",
       " '.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "PATTERN = r'''[\\.,;:]+|\\w+\\-\\w+|\\w+'''\n",
    "\n",
    "re.findall(PATTERN, 'Hoje é terça-feira. Segunda-feira fui ao médico e o Dr. Arnaldo em receitou um remédio.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "#### Criando casos especiais no Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_cases = {\n",
    "    'segunda-feira': [{ORTH: u'segunda-feira', LEMMA: u'segunda', POS: u'NOUN'}],\n",
    "    'terça-feira': [{ORTH: u'terça-feira', LEMMA: u'terça', POS: u'NOUN'}],\n",
    "    'quarta-feira': [{ORTH: u'quarta-feira', LEMMA: u'quarta', POS: u'NOUN'}],\n",
    "    'quinta-feira': [{ORTH: u'quinta-feira', LEMMA: u'quinta', POS: u'NOUN'}],\n",
    "    'sexta-feira': [{ORTH: u'sexta-feira', LEMMA: u'sexta', POS: u'NOUN'}],\n",
    "    'Segunda-feira': [{ORTH: u'Segunda-feira', LEMMA: u'Segunda', POS: u'NOUN'}],\n",
    "    'Terça-feira': [{ORTH: u'Terça-feira', LEMMA: u'Terça', POS: u'NOUN'}],\n",
    "    'Quarta-feira': [{ORTH: u'Quarta-feira', LEMMA: u'Quarta', POS: u'NOUN'}],\n",
    "    'Quinta-feira': [{ORTH: u'Quinta-feira', LEMMA: u'Quinta', POS: u'NOUN'}],\n",
    "    'Sexta-feira': [{ORTH: u'Sexta-feira', LEMMA: u'Sexta', POS: u'NOUN'}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segunda-feira [{65: 'segunda-feira', 73: 'segunda', 74: 'NOUN'}]\n",
      "terça-feira [{65: 'terça-feira', 73: 'terça', 74: 'NOUN'}]\n",
      "quarta-feira [{65: 'quarta-feira', 73: 'quarta', 74: 'NOUN'}]\n",
      "quinta-feira [{65: 'quinta-feira', 73: 'quinta', 74: 'NOUN'}]\n",
      "sexta-feira [{65: 'sexta-feira', 73: 'sexta', 74: 'NOUN'}]\n",
      "Segunda-feira [{65: 'Segunda-feira', 73: 'Segunda', 74: 'NOUN'}]\n",
      "Terça-feira [{65: 'Terça-feira', 73: 'Terça', 74: 'NOUN'}]\n",
      "Quarta-feira [{65: 'Quarta-feira', 73: 'Quarta', 74: 'NOUN'}]\n",
      "Quinta-feira [{65: 'Quinta-feira', 73: 'Quinta', 74: 'NOUN'}]\n",
      "Sexta-feira [{65: 'Sexta-feira', 73: 'Sexta', 74: 'NOUN'}]\n"
     ]
    }
   ],
   "source": [
    "def add_cases(dicio_cases, nlp_obj):\n",
    "    \n",
    "    for key, value in dicio_cases.items():\n",
    "        print(key, value)\n",
    "        nlp_obj.tokenizer.add_special_case(key, value)\n",
    "        \n",
    "add_cases(special_cases, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoje\n",
      "é\n",
      "terça-feira\n",
      ".\n",
      "Segunda-feira\n",
      "fui\n",
      "a\n",
      "o\n",
      "médico\n",
      "e\n",
      "o\n",
      "Dr.\n",
      "Arnaldo\n",
      "em\n",
      "receitou\n",
      "um\n",
      "remédio\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for n in nlp('Hoje é terça-feira. Segunda-feira fui ao médico e o Dr. Arnaldo em receitou um remédio.'):\n",
    "    print(n.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNLP:\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        assert isinstance(text, str), \"'A classe deve ser inicializada com uma string sendo passada por parâmetro'\"\n",
    "        \n",
    "        if text == '':\n",
    "            raise ValueError('A string passada não pode estar vazia')\n",
    "        \n",
    "        self.text = text\n",
    "        self.doc = nlp(self.text)\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        \n",
    "        return (t.text for t in self.doc)\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        tokens = self.words\n",
    "        \n",
    "        return \"Texto: %s\\nTokens: %s\\nNum Tokens: %d\" % (self.text, tokens, len(tokens))\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def words(self):\n",
    "            \n",
    "        return list(self.__iter__())\n",
    "    \n",
    "    \n",
    "    def words_preprocessing(self, stop_words_remove=True, punctuation_remove=True):\n",
    "        \n",
    "        tokens = list(self.__iter__())\n",
    "        \n",
    "        if stop_words_remove:\n",
    "            tokens = (tok for tok in tokens if tok not in stop_words_portuguese)\n",
    "        \n",
    "        elif punctuation_remove:\n",
    "            tokens = (tok for tok in tokens if tok not in punctuation)\n",
    "            \n",
    "        else:\n",
    "            tokens = (tok for tok in tokens)\n",
    "            \n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def sents(self):\n",
    "                \n",
    "        return (sent for sent in self.doc.sents)\n",
    "    \n",
    "    \n",
    "    def tokens_counter(self, ordered=True, reverse=True):\n",
    "        tok = list(self.__iter__())\n",
    "        \n",
    "        tok = OrderedDict(Counter(tok))\n",
    "        \n",
    "        if ordered:\n",
    "            tok = OrderedDict(sorted(tok.items(), key=lambda x: x[1], reverse=reverse))\n",
    "            \n",
    "        return tok\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TextNLP('Hoje é segunda-feira')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Texto: Hoje é segunda-feira\n",
       "Tokens: ['Hoje', 'é', 'segunda-feira']\n",
       "Num Tokens: 3"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

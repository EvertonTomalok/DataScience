{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma classe iterável, para tokenização e geração de sentenças \n",
    "    - Recupera tokens em português, que normalmente tokenizer de frameworks, falham em capturar, como por exemplo palavras compostas como dias da semana: terça-feira, segunda-feira; e outras palavras separadas por traço \"-\".\n",
    "    - Código com otimização de memória, utilizando geradores ao invés de armazenar valores na memória, utlizando funções geradoras (sintáxe muito similar a de list comprehension)\n",
    "        \n",
    "        - Sintáxe função geradora -> (x for x in 'some iterable here like a list, tuple, ...')\n",
    "        \n",
    "        - Exemplo de função geradora  def exemplo(): return (x for x in [1,2,3,4,5])\n",
    "        \n",
    "            - Exemplo utilização -> list( exemplo() ) -> Retorna a lista, sendo gerada de forma \"lazy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requisitos\n",
    "    \n",
    "    - pip install spacy \n",
    "    - python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.23) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "stop_words_portuguese = ['de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para', 'com', 'não', 'uma', 'os', 'no',\n",
    " 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou', 'quando', 'muito',\n",
    " 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos',\n",
    " 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa',\n",
    " 'pelos', 'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'vocês',\n",
    " 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas',\n",
    " 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos',\n",
    " 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos',\n",
    " 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem',\n",
    " 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam',\n",
    " 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos',\n",
    " 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi',\n",
    " 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos',\n",
    " 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha',\n",
    " 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham',\n",
    " 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria',\n",
    " 'teríamos', 'teriam']\n",
    "\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "PATTERN = r\"\"\"\\w+-+\\w+|\\w+|[!'\"#$%&\\*\\+,\\./:;\\?]{1}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextNLP:\n",
    "    \n",
    "    \"\"\"\n",
    "    Classe iterável, para recuperação de tokens e sentenças.\n",
    "    Exemplo de uso:\n",
    "        \n",
    "        # Inicializando a variável\n",
    "        >> w = TextNLP('Hoje é terça-feira, e o lobo é mau! E nós também somos! Somos? Será que somos...')\n",
    "        \n",
    "        # Visualizando o objeto criado\n",
    "        >> w\n",
    "          Texto: Hoje é terça-feira, e o lobo é mau! E nós também somos! Somos? Será que somos...\n",
    "          Tokens: ['Hoje', 'é', 'terça-feira', ',', 'e', 'o', 'lobo', 'é', 'mau', '!', 'E', 'nós', 'também', 'somos', '!', 'Somos', '?', 'Será', 'que', 'somos', '.', '.', '.']\n",
    "          Num Tokens: 23\n",
    "          \n",
    "        # Recuperando apenas os tokens\n",
    "        >> w.words\n",
    "          ['Hoje', 'é', 'terça-feira', ',', 'e', 'o', 'lobo', 'é', 'mau', '!', 'E', 'nós', 'também', 'somos', '!', 'Somos', '?', 'Será', 'que', 'somos', '.', '.', '.']\n",
    "    \n",
    "        # Preprocessando os tokens, retirando stop words, pontuação, ou ambos (Default)\n",
    "        >> list(w.words_preprocessing())\n",
    "          ['Hoje', 'é', 'terça-feira', 'lobo', 'é', 'mau', 'E', 'Somos', 'Será']\n",
    "\n",
    "        # Apenas stop words\n",
    "        >> list(w.words_preprocessing(punctuation_remove=False))\n",
    "          ['Hoje', 'é', 'terça-feira', ',', 'lobo', 'é', 'mau', '!', 'E', '!', 'Somos', '?', 'Será', '.', '.', '.']\n",
    "          \n",
    "        # Apenas pontuação\n",
    "        >> list(w.words_preprocessing(stop_words_remove=False))\n",
    "          ['Hoje', 'é', 'terça-feira', 'e', 'o', 'lobo', 'é', 'mau', 'E', 'nós', 'também', 'somos', 'Somos', 'Será', 'que', 'somos']\n",
    "\n",
    "        # Recuperação de sentenças\n",
    "        >> for sent in w.sents:\n",
    "           .... print(sent, ' <end sentence>')\n",
    "           \n",
    "           \n",
    "           \"\n",
    "               Hoje é terça-feira, e o lobo é mau!  <end sentence>\n",
    "               E nós também somos!  <end sentence>\n",
    "               Somos?  <end sentence>\n",
    "               Será que somos...  <end sentence>\n",
    "           \"   \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, text):\n",
    "        assert isinstance(text, str), \"'A classe deve ser inicializada com uma string sendo passada por parâmetro'\"\n",
    "        \n",
    "        if text == '':\n",
    "            raise ValueError('A string passada não pode estar vazia')\n",
    "        \n",
    "        self.text = text\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        \n",
    "        return (match.group() for match in re.finditer(PATTERN, self.text))\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        tokens = self.words\n",
    "        \n",
    "        return \"Texto: %s\\nTokens: %s\\nNum Tokens: %d\" % (self.text, tokens, len(tokens))\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def words(self):\n",
    "            \n",
    "        return list(self.__iter__())\n",
    "    \n",
    "    \n",
    "    def words_preprocessing(self, stop_words_remove=True, punctuation_remove=True):\n",
    "        \n",
    "        tokens = list(self.__iter__())\n",
    "        \n",
    "        if stop_words_remove:\n",
    "            tokens = (tok for tok in tokens if tok not in stop_words_portuguese)\n",
    "        \n",
    "        if punctuation_remove:\n",
    "            tokens = (tok for tok in tokens if tok not in punctuation)\n",
    "            \n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def sents(self):\n",
    "        doc = nlp(self.text)\n",
    "                \n",
    "        return (sent for sent in doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hoje',\n",
       " 'é',\n",
       " 'terça-feira',\n",
       " 'e',\n",
       " 'o',\n",
       " 'lobo',\n",
       " 'é',\n",
       " 'mau',\n",
       " 'E',\n",
       " 'nós',\n",
       " 'também',\n",
       " 'somos',\n",
       " 'Somos',\n",
       " 'Será',\n",
       " 'que',\n",
       " 'somos']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = TextNLP('Hoje é terça-feira, e o lobo é mau! E nós também somos! Somos? Será que somos...')\n",
    "list(w.words_preprocessing(stop_words_remove=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hoje',\n",
       " 'é',\n",
       " 'terça-feira',\n",
       " ',',\n",
       " 'lobo',\n",
       " 'é',\n",
       " 'mau',\n",
       " '!',\n",
       " 'E',\n",
       " '!',\n",
       " 'Somos',\n",
       " '?',\n",
       " 'Será',\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(w.words_preprocessing(punctuation_remove=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hoje', 'é', 'terça-feira', 'lobo', 'é', 'mau', 'E', 'Somos', 'Será']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default\n",
    "list(w.words_preprocessing())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Texto: Hoje é terça-feira, e o lobo é mau! E nós também somos! Somos? Será que somos...\n",
       "Tokens: ['Hoje', 'é', 'terça-feira', ',', 'e', 'o', 'lobo', 'é', 'mau', '!', 'E', 'nós', 'também', 'somos', '!', 'Somos', '?', 'Será', 'que', 'somos', '.', '.', '.']\n",
       "Num Tokens: 23"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hoje',\n",
       " 'é',\n",
       " 'terça-feira',\n",
       " ',',\n",
       " 'e',\n",
       " 'o',\n",
       " 'lobo',\n",
       " 'é',\n",
       " 'mau',\n",
       " '!',\n",
       " 'E',\n",
       " 'nós',\n",
       " 'também',\n",
       " 'somos',\n",
       " '!',\n",
       " 'Somos',\n",
       " '?',\n",
       " 'Será',\n",
       " 'que',\n",
       " 'somos',\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoje\n",
      "é\n",
      "terça-feira\n",
      ",\n",
      "e\n",
      "o\n",
      "lobo\n",
      "é\n",
      "mau\n",
      "!\n",
      "E\n",
      "nós\n",
      "também\n",
      "somos\n",
      "!\n",
      "Somos\n",
      "?\n",
      "Será\n",
      "que\n",
      "somos\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for item in w.words:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoje é terça-feira, e o lobo é mau!  <end sentence>\n",
      "E nós também somos!  <end sentence>\n",
      "Somos?  <end sentence>\n",
      "Será que somos...  <end sentence>\n"
     ]
    }
   ],
   "source": [
    "for sent in w.sents:\n",
    "    print(sent, ' <end sentence>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
